# VDHGAN - Implementation
## Vulnerability Detection Based on Heterogeneous Graph Attention Network

## Introduction
Vulnerability detection is crucial for mitigating inherent risks in software systems. Recent years have
seen a surge in the development of effective vulnerability detection methods, leveraging deep learning
technologies to offer advantages such as automated feature extraction and automatic model training.
However, existing methods still face two major limitations. Firstly, code analysis lacks granularity
and fails to fully leverage the semantic and syntactic information within code structure graphs,
resulting in suboptimal performance. Secondly, Graph Neural Network (GNN)-based approaches
inherently struggle to capture long-distance relationships between nodes in code structure graphs.
To address these limitations, we propose VDHGAN, a new vulnerability detection method based
on heterogeneous intermediate source code representations. VDHGAN consists primarily of two
components: a heterogeneous code representation graph, constructed by establishing heterogeneous
code representation and simplifying the graph to reduce node distances, and a Heterogeneous Graph
Attention Network, comprising two attention layers that calculate node-level attention and semantic-
level attention. Experiments on three common benchmark datasets demonstrate that VDHGAN
outperforms state-of-the-art methods by 1.5% to 7.7% in accuracy and 3.8% to 12.2% in F1 score
metrics, affirming its effectiveness in learning global information for code graphs used in vulnerability
detection.

## Dataset
To investigate the effectiveness of VDHGAN, we adopt three vulnerability datasets from these paper: 
* Devign: <https://drive.google.com/file/d/1x6hoF7G-tSYxg8AFybggypLZgMGDNHfF>
* Fan et al.: <https://drive.google.com/file/d/1-0VhnHBp9IGh90s2wCNjeCMuy70HPl8X/view?usp=sharing>
* Reveal: <https://drive.google.com/drive/folders/1KuIYgFcvWUXheDhT--cBALsfy1I4utOy>


## Requirement
Our code is based on Python3 (>= 3.7). There are a few dependencies to run the code. The major libraries are listed as follows:
* torch  (==1.9.0)
* dgl  (==0.7.2)
* numpy  (==1.22.3)
* sklearn  (==0.0)
* pandas  (==1.4.1)
* tqdm
* pickle

**Default settings in VDHGAN**:
* Training configs: 
    * batch_size = 64, lr = 0.0001, epoch = 100, patience = 20
    * opt ='RAdam', weight_decay=1e-6

## Preprocessing
We use Joern to generate the code structure graph. It should be noted that the AST and graphs generated by different versions of Joern may have significant differences. So if using the newer versions of Joern to generate code structure graph, the model may have a different performance compared with the results we reported in the paper.

After parsing the functions with joern, the code for graph construction and simplification is under the ```data_processing``` folder. ```data_processing\word2vec.py``` is used to train word2vec model.

Follow the ```data_processing\readme.md``` to preprocess the dataset.
## Running the model
The model implementation code is under the ``` VDHGAN_code``` folder. The model can be runned from ```VDHGAN_code\main.py```.


